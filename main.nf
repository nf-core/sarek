#!/usr/bin/env nextflow

/*
================================================================================
                                  nf-core/sarek
================================================================================
Started March 2016.
Ported to nf-core May 2019.
--------------------------------------------------------------------------------
nf-core/sarek:
  An open-source analysis pipeline to detect germline or somatic variants
  from whole genome or targeted sequencing
--------------------------------------------------------------------------------
 @Homepage
 https://sarek.scilifelab.se/
--------------------------------------------------------------------------------
 @Documentation
 https://github.com/nf-core/sarek/README.md
--------------------------------------------------------------------------------
*/

def helpMessage() {
    // TODO nf-core: Add to this help message with new command line parameters
    log.info nfcoreHeader()
    log.info"""

    Usage:

    The typical command for running the pipeline is as follows:

    nextflow run nf-core/sarek --sample sample.tsv -profile docker

    Mandatory arguments:
        --sample                    Path to TSV input file
                                    Multiple TSV files can be specified with quotes
                                    Works also with a directory on mapping step with germline sample only
        -profile                    Configuration profile to use. Can use multiple (comma separated)
                                    Available: conda, docker, singularity, awsbatch, test and more.

    Options:
        --genome                    Name of iGenomes reference
        --noGVCF                    no g.vcf output from HaplotypeCaller
        --noReports                 disable QC reports
        --nucleotidesPerSecond      To estimate interval size by default 1000.0
        --sequencing_center         Name of sequencing center to be displayed in BAM file
        --step                      Specify starting step
                                    Available: Mapping, Recalibrate, VariantCalling
                                    Default: Mapping
        --strelkaBP                 Use Manta candidateSmallIndels for Strelka as Best Practice
        --targetBED                 target BED file for targeted sequencing
        --tools                     Specify tools to use for variant calling
                                    Available: ASCAT, ControlFREEC, FreeBayes, HaplotypeCaller
                                    Manta, mpileup, MuTect2, Strelka

    References                      If not specified in the configuration file or you wish to overwrite any of the references.
        --acLoci                    acLoci file
        --acLociGC                  acLoci GC file
        --bwaIndex                  bwa indexes
        --dbsnp                     dbsnp file
        --dbsnpIndex                dbsnp index
        --genomeDict                genome dict
        --genomeFile                genome file
        --genomeIndex               genome index
        --intervals                 intervals
        --knownIndels               knownIndels file
        --knownIndelsIndex          knownIndels index
        --snpeffDb                  snpeffDb version
        --vepCacheVersion           VEP Cache version

    Other options:
        --outdir                    The output directory where the results will be saved
        --email                     Set this parameter to your e-mail address to get a summary e-mail with details of the run sent to you when the workflow exits
        --maxMultiqcEmailFileSize   Theshold size for MultiQC report to be attached in notification email. If file generated by pipeline exceeds the threshold, it will not be attached (Default: 25MB)
        -name                       Name for the pipeline run. If not specified, Nextflow will automatically generate a random mnemonic

    AWSBatch options:
        --awsqueue                  The AWSBatch JobQueue that needs to be set when running on AWSBatch
        --awsregion                 The AWS Region for your AWS Batch job to run on
    """.stripIndent()
}

/*
 * SET UP CONFIGURATION VARIABLES
 */

// Show help message
if (params.help) exit 0, helpMessage()

// Check if genome exists in the config file
if (params.genomes && params.genome && !params.genomes.containsKey(params.genome)) {
    exit 1, "The provided genome '${params.genome}' is not available in the iGenomes file. Currently the available genomes are ${params.genomes.keySet().join(", ")}"
}

// Default value for params
params.annotateTools = null
params.annotateVCF = null
params.annotation_cache = null
params.cadd_InDels = null
params.cadd_InDels_tbi = null
params.cadd_WG_SNVs = null
params.cadd_WG_SNVs_tbi = null
params.noGVCF = null
params.noReports = null
params.nucleotidesPerSecond = 1000.0
params.sample = null
params.sequencing_center = null
params.step = 'mapping'
params.strelkaBP = true
params.targetBED = null
params.tools = null

stepList = defineStepList()
step = params.step ? params.step.toLowerCase() : ''
if ( step == 'preprocessing' ) step = 'mapping'
if ( !checkParameterExistence(step, stepList) ) exit 1, 'Unknown step, see --help for more information'
if ( step.contains(',') ) exit 1, 'You can choose only one step, see --help for more information'

tools = params.tools ? params.tools.split(',').collect{it.trim().toLowerCase()} : []
annotateTools = params.annotateTools ? params.annotateTools.split(',').collect{it.trim().toLowerCase()} : []
annotateVCF = params.annotateVCF ? params.annotateVCF.split(',').collect{it.trim()} : []
toolList = defineToolList()
if ( !checkParameterList(tools,toolList) ) exit 1, 'Unknown tool(s), see --help for more information'

referenceMap = defineReferenceMap(step, tools)
if ( !checkReferenceMap(referenceMap) ) exit 1, 'Missing Reference file(s), see --help for more information'

// Has the run name been specified by the user?
// this has the bonus effect of catching both -name and --name
custom_runName = params.name
if ( !(workflow.runName ==~ /[a-z]+_[a-z]+/) ) custom_runName = workflow.runName

if ( workflow.profile == 'awsbatch') {
    // AWSBatch sanity checking
    if ( !params.awsqueue || !params.awsregion ) exit 1, "Specify correct --awsqueue and --awsregion parameters on AWSBatch!"
    // Check outdir paths to be S3 buckets if running on AWSBatch
    // related: https://github.com/nextflow-io/nextflow/issues/813
    if ( !params.outdir.startsWith('s3:') ) exit 1, "Outdir not on S3 - specify S3 Bucket to run on AWSBatch!"
    // Prevent trace files to be stored on S3 since S3 does not support rolling files.
    if ( workflow.tracedir.startsWith('s3:') ) exit 1, "Specify a local tracedir or run without trace! S3 cannot be used for tracefiles."
}

// Stage config files
ch_multiqc_config = Channel.fromPath(params.multiqc_config)
ch_output_docs = Channel.fromPath("${baseDir}/docs/output.md")

/*
 * Create a channel for input read files
 */

tsvPath = null
if (params.sample) if (hasExtension(params.sample,"tsv")) tsvPath = params.sample

 // No need for tsv file for step annotate
if (!params.sample) {
    tsvPaths = [    'recalibrate':    "${params.outdir}/Preprocessing/TSV/duplicateMarked.tsv",
                    'variantcalling': "${params.outdir}/Preprocessing/TSV/recalibrated.tsv" ]
    if (step != 'mapping') tsvPath = tsvPaths[step]
}

// Set up the inputFiles and bamFiles channels. One of them will remain empty
inputFiles = Channel.empty()
bamFiles = Channel.empty()
if (tsvPath) {
    tsvFile = file(tsvPath)
    switch (step) {
        case 'mapping': inputFiles = extractSample(tsvFile); break
        case 'recalibrate': bamFiles = extractRecal(tsvFile); break
        case 'variantcalling': bamFiles = extractBams(tsvFile); break
        default: exit 1, "Unknown step ${step}"
    }
} else if (params.sample) if (!hasExtension(params.sample,"tsv")) {
    println "no tsv file"
    if (step != 'mapping') exit 1, 'No other step than "mapping" support a dir as an input'
    inputFiles = extractFastqFromDir(params.sample)
    (inputFiles, fastqTmp) = inputFiles.into(2)
    fastqTmp.toList().subscribe onNext: {
    if (it.size() == 0) exit 1, "No FASTQ files found in --sample directory '${params.sample}'"
}
tsvFile = params.sample  // used in the reports
} else exit 1, 'No sample were defined, see --help'

if (step == 'recalibrate') (patientGenders, bamFiles) = extractGenders(bamFiles)
else (patientGenders, inputFiles) = extractGenders(inputFiles)

// Header log info
log.info nfcoreHeader()
def summary = [:]
if (workflow.revision)  summary['Pipeline Release'] = workflow.revision
summary['Run Name']         = custom_runName ?: workflow.runName
summary['Max Resources']    = "${params.max_memory} memory, ${params.max_cpus} cpus, ${params.max_time} time per job"
if (workflow.containerEngine)   summary['Container']        = "${workflow.containerEngine} - ${workflow.container}"
if (params.sample)              summary['Sample']           = params.sample
if (params.targetBED)           summary['Target BED']       = params.targetBED
if (params.step)                summary['Step']             = params.step
if (params.tools)               summary['Tools']            = tools.join(', ')
if (params.noReports)           summary['Reports']          = params.noReports
if (params.noGVCF)              summary['GVCF']             = params.noGVCF
if (params.strelkaBP)           summary['Strelka BP']       = params.strelkaBP
if (params.sequencing_center)   summary['Sequenced by ']    = params.sequencing_center
summary['Nucleotides/s']    = params.nucleotidesPerSecond
summary['Output dir']       = params.outdir
summary['Launch dir']       = workflow.launchDir
summary['Working dir']      = workflow.workDir
summary['Script dir']       = workflow.projectDir
summary['User']             = workflow.userName
if (workflow.profile == 'awsbatch'){
    summary['AWS Region']       = params.awsregion
    summary['AWS Queue']        = params.awsqueue
}
summary['Config Profile'] = workflow.profile
if (params.config_profile_description)  summary['Config Description'] = params.config_profile_description
if (params.config_profile_contact)      summary['Config Contact']     = params.config_profile_contact
if (params.config_profile_url)          summary['Config URL']         = params.config_profile_url
if (params.email) {
    summary['E-mail Address']       = params.email
    summary['MultiQC maxsize']      = params.maxMultiqcEmailFileSize
}
log.info summary.collect { k,v -> "${k.padRight(18)}: $v" }.join("\n")
log.info "\033[2m----------------------------------------------------\033[0m"

// Check the hostnames against configured profiles
checkHostname()

def create_workflow_summary(summary) {
    def yaml_file = workDir.resolve('workflow_summary_mqc.yaml')
    yaml_file.text  = """
    id: 'nf-core-sarek-summary'
    description: " - this information is collected when the pipeline is started."
    section_name: 'nf-core/sarek Workflow Summary'
    section_href: 'https://github.com/nf-core/sarek'
    plot_type: 'html'
    data: |
        <dl class=\"dl-horizontal\">
${summary.collect { k,v -> "            <dt>$k</dt><dd><samp>${v ?: '<span style=\"color:#999999;\">N/A</a>'}</samp></dd>" }.join("\n")}
        </dl>
    """.stripIndent()

   return yaml_file
}

/*
 * Parse software version numbers
 */
process get_software_versions {
    publishDir path:"${params.outdir}/pipeline_info", mode: params.publishDirMode

    output:
        file 'software_versions_mqc.yaml' into software_versions_yaml

    script:
    """
    alleleCounter --version &> v_allelecount.txt  || true
    bcftools version > v_bcftools.txt 2>&1 || true
    bwa &> v_bwa.txt 2>&1 || true
    cat ${baseDir}/scripts/ascat.R | grep "ASCAT version" &> v_ascat.txt  || true
    configManta.py --version > v_manta.txt 2>&1 || true
    configureStrelkaGermlineWorkflow.py --version > v_strelka.txt 2>&1 || true
    echo "${workflow.manifest.version}" &> v_pipeline.txt 2>&1 || true
    echo "${workflow.nextflow.version}" &> v_nextflow.txt 2>&1 || true
    echo "SNPEFF version"\$(snpEff -h 2>&1) > v_snpeff.txt
    fastqc --version > v_fastqc.txt 2>&1 || true
    freebayes --version > v_freebayes.txt 2>&1 || true
    gatk ApplyBQSR --help 2>&1 | grep Version: > v_gatk.txt 2>&1 || true
    multiqc --version &> v_multiqc.txt 2>&1 || true
    qualimap --version &> v_qualimap.txt 2>&1 || true
    R --version &> v_r.txt  || true
    samtools --version &> v_samtools.txt 2>&1 || true
    vcftools --version &> v_vcftools.txt 2>&1 || true
    vep --help &> v_vep.txt 2>&1 || true

    scrape_software_versions.py &> software_versions_mqc.yaml
    """
}

software_versions_yaml = software_versions_yaml.dump(tag: 'SOFTWARE VERSIONS')

/*
================================================================================
                                  PREPROCESSING
================================================================================
*/

// STEP ONE: MAPPING

(inputFiles, inputFilesforFastQC) = inputFiles.into(2)

inputFiles = inputFiles.dump(tag:'INPUT')

process RunFastQC {
    tag {idPatient + "-" + idRun}

    publishDir "${params.outdir}/Reports/${idSample}/FastQC/${idRun}", mode: params.publishDirMode

    input:
        set idPatient, status, idSample, idRun, file(inputFile1), file(inputFile2) from inputFilesforFastQC

    output:
        file "*_fastqc.{zip,html}" into fastQCreport

    when: step == 'mapping' && !params.noReports

    script:
    inputFiles = (hasExtension(inputFile1,"fastq.gz") || hasExtension(inputFile1,"fq.gz")) ? "${inputFile1} ${inputFile2}" : "${inputFile1}"
    """
    fastqc -t 2 -q ${inputFiles}
    """
}

fastQCreport = fastQCreport.dump(tag:'FastQC')

process MapReads {
    tag {idPatient + "-" + idRun}

    input:
        set idPatient, status, idSample, idRun, file(inputFile1), file(inputFile2) from inputFiles
        set file(genomeFile), file(bwaIndex) from Channel.value([referenceMap.genomeFile, referenceMap.bwaIndex])

    output:
        set idPatient, status, idSample, idRun, file("${idRun}.bam") into (mappedBam, mappedBamForQC)

    when: step == 'mapping'

    script:
    // -K is an hidden option, used to fix the number of reads processed by bwa mem
    // Chunk size can affect bwa results, if not specified, the number of threads can change
    // which can give not deterministic result.
    // cf https://github.com/CCDG/Pipeline-Standardization/blob/master/PipelineStandard.md
    // and https://github.com/gatk-workflows/gatk4-data-processing/blob/8ffa26ff4580df4ac3a5aa9e272a4ff6bab44ba2/processing-for-variant-discovery-gatk4.b37.wgs.inputs.json#L29
    CN = params.sequencing_center ? "CN:${params.sequencing_center}\\t" : ""
    readGroup = "@RG\\tID:${idRun}\\t${CN}PU:${idRun}\\tSM:${idSample}\\tLB:${idSample}\\tPL:illumina"
    // adjust mismatch penalty for tumor samples
    extra = status == 1 ? "-B 3" : ""
    if (hasExtension(inputFile1,"fastq.gz") || hasExtension(inputFile1,"fq.gz"))
    """
        bwa mem -K 100000000 -R \"${readGroup}\" ${extra} -t ${task.cpus} -M \
            ${genomeFile} ${inputFile1} ${inputFile2} | \
        samtools sort --threads ${task.cpus} -m 2G - > ${idRun}.bam
    """
    else if (hasExtension(inputFile1,"bam"))
    """
        gatk --java-options -Xmx${task.memory.toGiga()}g \
            SamToFastq \
            --INPUT=${inputFile1} \
            --FASTQ=/dev/stdout \
            --INTERLEAVE=true \
            --NON_PF=true \
            | \
        bwa mem -K 100000000 -p -R \"${readGroup}\" ${extra} -t ${task.cpus} -M ${genomeFile} \
            /dev/stdin - 2> >(tee ${inputFile1}.bwa.stderr.log >&2) \
            | \
        samtools sort --threads ${task.cpus} -m 2G - > ${idRun}.bam
    """
}

mappedBam = mappedBam.dump(tag:'Mapped BAM')

process RunBamQCmapped {
    tag {idPatient + "-" + idSample}

    publishDir "${params.outdir}/Reports/${idSample}/bamQC", mode: params.publishDirMode

    input:
        set idPatient, status, idSample, idRun, file(bam) from mappedBamForQC
        file(targetBED) from Channel.value(params.targetBED ? file(params.targetBED) : "null")

    output:
        file("${bam.baseName}") into bamQCmappedReport

    when: !params.noReports

    script:
    use_bed = params.targetBED ? "-gff ${targetBED}" : ''
    """
    qualimap --java-mem-size=${task.memory.toGiga()}G \
        bamqc \
        -bam ${bam} \
        --paint-chromosome-limits \
        --genome-gc-distr HUMAN \
        $use_bed \
        -nt ${task.cpus} \
        -skip-duplicated \
        --skip-dup-mode 0 \
        -outdir ${bam.baseName} \
        -outformat HTML
    """
}

bamQCmappedReport = bamQCmappedReport.dump(tag:'BamQC BAM')

// Sort bam whether they are standalone or should be merged

singleBam = Channel.create()
groupedBam = Channel.create()
mappedBam.groupTuple(by:[0,1,2])
    .choice(singleBam, groupedBam) {it[3].size() > 1 ? 1 : 0}
singleBam = singleBam.map {
    idPatient, status, idSample, idRun, bam ->
    [idPatient, status, idSample, bam]
}

process MergeBams {
    tag {idPatient + "-" + idSample}

    input:
        set idPatient, status, idSample, idRun, file(bam) from groupedBam

    output:
        set idPatient, status, idSample, file("${idSample}.bam") into mergedBam

    when: step == 'mapping'

    script:
    """
    samtools merge --threads ${task.cpus} ${idSample}.bam ${bam}
    """
}

singleBam = singleBam.dump(tag:'Single BAM')
mergedBam = mergedBam.dump(tag:'Merged BAM')
mergedBam = mergedBam.mix(singleBam)
mergedBam = mergedBam.dump(tag:'BAM for MD')

process MarkDuplicates {
    tag {idPatient + "-" + idSample}

    publishDir params.outdir, mode: params.publishDirMode,
        saveAs: {
            if (it == "${idSample}.bam.metrics") "Reports/${idSample}/MarkDuplicates/${it}"
            else "Preprocessing/${idSample}/DuplicateMarked/${it}"
        }

    input:
        set idPatient, status, idSample, file("${idSample}.bam") from mergedBam

    output:
        set idPatient, file("${idSample}_${status}.md.bam"), file("${idSample}_${status}.md.bai") into duplicateMarkedBams
        file ("${idSample}.bam.metrics") into markDuplicatesReport

    when: step == 'mapping'

    script:
    markdup_java_options = task.memory.toGiga() > 8 ? params.markdup_java_options : "\"-Xms" +  (task.memory.toGiga() / 2 ).trunc() + "g -Xmx" + (task.memory.toGiga() - 1) + "g\""
    """
    gatk --java-options ${markdup_java_options} \
        MarkDuplicates \
        --MAX_RECORDS_IN_RAM 50000 \
        --INPUT ${idSample}.bam \
        --METRICS_FILE ${idSample}.bam.metrics \
        --TMP_DIR . \
        --ASSUME_SORT_ORDER coordinate \
        --CREATE_INDEX true \
        --OUTPUT ${idSample}_${status}.md.bam
    """
}

duplicateMarkedBams = duplicateMarkedBams.map {
    idPatient, bam, bai ->
    tag = bam.baseName.tokenize('.')[0]
    status   = tag[-1..-1].toInteger()
    idSample = tag.take(tag.length()-2)
    [idPatient, status, idSample, bam, bai]
}

duplicateMarkedBams = duplicateMarkedBams.dump(tag:'MD BAM')

(mdBam, mdBamToJoin) = duplicateMarkedBams.into(2)

process CreateIntervalBeds {
    tag {intervals.fileName}

    input:
        file(intervals) from Channel.value(referenceMap.intervals)

    output:
        file '*.bed' into bedIntervals mode flatten

    when: step != 'annotate'

    script:
    // If the interval file is BED format, the fifth column is interpreted to
    // contain runtime estimates, which is then used to combine short-running jobs
    if (hasExtension(intervals,"bed"))
        """
        awk -vFS="\t" '{
          t = \$5  # runtime estimate
          if (t == "") {
            # no runtime estimate in this row, assume default value
            t = (\$3 - \$2) / ${params.nucleotidesPerSecond}
          }
          if (name == "" || (chunk > 600 && (chunk + t) > longest * 1.05)) {
            # start a new chunk
            name = sprintf("%s_%d-%d.bed", \$1, \$2+1, \$3)
            chunk = 0
            longest = 0
          }
          if (t > longest)
            longest = t
          chunk += t
          print \$0 > name
        }' ${intervals}
        """
    else
        """
        awk -vFS="[:-]" '{
          name = sprintf("%s_%d-%d", \$1, \$2, \$3);
          printf("%s\\t%d\\t%d\\n", \$1, \$2-1, \$3) > name ".bed"
        }' ${intervals}
        """
}

bedIntervals = bedIntervals
    .map { intervalFile ->
        def duration = 0.0
        for (line in intervalFile.readLines()) {
            final fields = line.split('\t')
            if (fields.size() >= 5) duration += fields[4].toFloat()
            else {
                start = fields[1].toInteger()
                end = fields[2].toInteger()
                duration += (end - start) / params.nucleotidesPerSecond
            }
        }
        [duration, intervalFile]
        }.toSortedList({ a, b -> b[0] <=> a[0] })
    .flatten().collate(2)
    .map{duration, intervalFile -> intervalFile}

bedIntervals = bedIntervals.dump(tag:'bedintervals')

(bedIntervalsBR, bedIntervalsHC, bedIntervalsForMpileup, bedIntervals) = bedIntervals.into(4)

bamForBaseRecalibrator = mdBam.combine(bedIntervalsBR)

process CreateRecalibrationTable {
    tag {idPatient + "-" + idSample + "-" + intervalBed}

    publishDir "${params.outdir}/Preprocessing/${idSample}/DuplicateMarked", mode: params.publishDirMode, overwrite: false

    input:
        set idPatient, status, idSample, file(bam), file(bai), file(intervalBed) from bamForBaseRecalibrator
        set file(genomeFile), file(genomeIndex), file(genomeDict), file(dbsnp), file(dbsnpIndex), file(knownIndels), file(knownIndelsIndex) from Channel.value([
            referenceMap.genomeFile,
            referenceMap.genomeIndex,
            referenceMap.genomeDict,
            referenceMap.dbsnp,
            referenceMap.dbsnpIndex,
            referenceMap.knownIndels,
            referenceMap.knownIndelsIndex,
        ])

    output:
        set idPatient, status, idSample, file("${intervalBed.baseName}_${idSample}.recal.table") into recalIntervals

    when: step == 'mapping'

    script:
    known = knownIndels.collect{ "--known-sites ${it}" }.join(' ')
    // --use-original-qualities ???
    """
    gatk --java-options -Xmx${task.memory.toGiga()}g \
        BaseRecalibrator \
        -I ${bam} \
        -O ${intervalBed.baseName}_${idSample}.recal.table \
        --tmp-dir /tmp \
        -R ${genomeFile} \
        -L ${intervalBed} \
        --known-sites ${dbsnp} \
        ${known} \
        --verbosity INFO
    """
}

recalIntervals = recalIntervals.groupTuple(by:[0,1,2])

process GatherBQSRReports {
    tag {idPatient + "-" + idSample}

    publishDir "${params.outdir}/Preprocessing/${idSample}/DuplicateMarked", mode: params.publishDirMode, overwrite: false

    input:
    set idPatient, status, idSample, file(recalTable) from recalIntervals

    output:
    set idPatient, status, idSample, file("${idSample}.recal.table") into recalibrationTable
    set idPatient, status, idSample, val("${idSample}_${status}.md.bam"), val("${idSample}_${status}.md.bai"), val("${idSample}.recal.table") into (recalibrationTableTSV, recalibrationTableSampleTSV)

    when: step == 'mapping'

    script:
    recal = recalTable.collect{ "-I ${it}" }.join(' ')
    """
    gatk --java-options -Xmx${task.memory.toGiga()}g \
        GatherBQSRReports \
        ${recal} \
        -O ${idSample}.recal.table \
    """
}

// Create TSV files to restart from this step
recalibrationTableTSV.map { idPatient, status, idSample, bam, bai, recalTable ->
    gender = patientGenders[idPatient]
    "${idPatient}\t${gender}\t${status}\t${idSample}\t${params.outdir}/Preprocessing/${idSample}/DuplicateMarked/${bam}\t${params.outdir}/Preprocessing/${idSample}/DuplicateMarked/${bai}\t${params.outdir}/Preprocessing/${idSample}/DuplicateMarked/${recalTable}\n"
}.collectFile(
    name: 'duplicateMarked.tsv', sort: true, storeDir: "${params.outdir}/Preprocessing/TSV"
)

recalibrationTableSampleTSV
    .collectFile(storeDir: "${params.outdir}/Preprocessing/TSV") {
        idPatient, status, idSample, bam, bai, recalTable ->
        gender = patientGenders[idPatient]
        ["duplicateMarked_${idSample}.tsv", "${idPatient}\t${gender}\t${status}\t${idSample}\t${params.outdir}/Preprocessing/${idSample}/DuplicateMarked/${bam}\t${params.outdir}/Preprocessing/${idSample}/DuplicateMarked/${bai}\t${params.outdir}/Preprocessing/${idSample}/DuplicateMarked/${recalTable}\n"]
}

recalibrationTable = mdBamToJoin.join(recalibrationTable, by:[0,1,2])

if (step == 'recalibrate') recalibrationTable = bamFiles

recalibrationTable = recalibrationTable.dump(tag:'recal.table')

process RecalibrateBam {
    tag {idPatient + "-" + idSample}

    publishDir "${params.outdir}/Preprocessing/${idSample}/Recalibrated", mode: params.publishDirMode

    input:
        set idPatient, status, idSample, file(bam), file(bai), file(recalibrationReport) from recalibrationTable
        set file(genomeFile), file(genomeIndex), file(genomeDict), file(intervals) from Channel.value([
            referenceMap.genomeFile,
            referenceMap.genomeIndex,
            referenceMap.genomeDict,
            referenceMap.intervals,
        ])

    output:
        set idPatient, status, idSample, file("${idSample}.recal.bam"), file("${idSample}.recal.bai") into recalibratedBam, recalibratedBamForStats
        set idPatient, status, idSample, val("${idSample}.recal.bam"), val("${idSample}.recal.bai") into (recalibratedBamTSV, recalibratedBamSampleTSV)

    script:
    """
    gatk --java-options -Xmx${task.memory.toGiga()}g \
        ApplyBQSR \
        -R ${genomeFile} \
        --input ${bam} \
        --output ${idSample}.recal.bam \
        -L ${intervals} \
        --create-output-bam-index true \
        --bqsr-recal-file ${recalibrationReport}
    """
}

// Creating a TSV file to restart from this step
recalibratedBamTSV.map { idPatient, status, idSample, bam, bai ->
    gender = patientGenders[idPatient]
    "${idPatient}\t${gender}\t${status}\t${idSample}\t${params.outdir}/Preprocessing/${idSample}/Recalibrated/${bam}\t${params.outdir}/Preprocessing/${idSample}/Recalibrated/${bai}\n"
}.collectFile(
    name: 'recalibrated.tsv', sort: true, storeDir: "${params.outdir}/Preprocessing/TSV"
)

recalibratedBamSampleTSV
    .collectFile(storeDir: "${params.outdir}/Preprocessing/TSV") {
        idPatient, status, idSample, bam, bai ->
        gender = patientGenders[idPatient]
        ["recalibrated_${idSample}.tsv", "${idPatient}\t${gender}\t${status}\t${idSample}\t${params.outdir}/Preprocessing/${idSample}/Recalibrated/${bam}\t${params.outdir}/Preprocessing/${idSample}/Recalibrated/${bai}\n"]
}

recalibratedBam = recalibratedBam.dump(tag:'recal.bam')

// Remove recalTable from Channels to match inputs for Process to avoid:
// WARN: Input tuple does not match input set cardinality declared by process...
(bamForBamQC, bamForSamToolsStats) = recalibratedBamForStats.map{ it[0..4] }.into(2)

process RunSamtoolsStats {
    tag {idPatient + "-" + idSample}

    publishDir "${params.outdir}/Reports/${idSample}/SamToolsStats", mode: params.publishDirMode

    input:
    set idPatient, status, idSample, file(bam), file(bai) from bamForSamToolsStats

    output:
    file ("${bam}.samtools.stats.out") into samtoolsStatsReport

    when: !params.noReports

    script:
    """
    samtools stats ${bam} > ${bam}.samtools.stats.out
    """
}

samtoolsStatsReport = samtoolsStatsReport.dump(tag:'SAMTools')

process RunBamQCrecalibrated {
    tag {idPatient + "-" + idSample}

    publishDir "${params.outdir}/Reports/${idSample}/bamQC", mode: params.publishDirMode

    input:
    set idPatient, status, idSample, file(bam), file(bai) from bamForBamQC

    output:
    file("${bam.baseName}") into bamQCrecalibratedReport

    when: !params.noReports

    script:
    """
    qualimap --java-mem-size=${task.memory.toGiga()}G \
        bamqc \
        -bam ${bam} \
        --paint-chromosome-limits \
        --genome-gc-distr HUMAN \
        -nt ${task.cpus} \
        -skip-duplicated \
        --skip-dup-mode 0 \
        -outdir ${bam.baseName} \
        -outformat HTML
    """
}

bamQCrecalibratedReport = bamQCrecalibratedReport.dump(tag:'BamQC')

/*
================================================================================
                            GERMLINE VARIANT CALLING
================================================================================
*/

if (step == 'variantcalling') recalibratedBam = bamFiles

recalibratedBam = recalibratedBam.dump(tag:'BAM')

// Here we have a recalibrated bam set
// The TSV file is formatted like: "idPatient status idSample bamFile baiFile"
// Manta will be run in Germline mode, or in Tumor mode depending on status
// HaplotypeCaller and Strelka will be run for Normal and Tumor samples

(bamsForSingleManta, bamsForSingleStrelka, recalibratedBamTemp, recalibratedBam) = recalibratedBam.into(4)

// To speed Variant Callers up we are chopping the reference into smaller pieces
// Do variant calling by this intervals, and re-merge the VCFs

bamsForHC = recalibratedBamTemp.combine(bedIntervalsHC)

process RunHaplotypecaller {
    tag {idSample + "-" + intervalBed.baseName}

    input:
        set idPatient, status, idSample, file(bam), file(bai), file(intervalBed) from bamsForHC
        set file(genomeFile), file(genomeIndex), file(genomeDict), file(dbsnp), file(dbsnpIndex) from Channel.value([
            referenceMap.genomeFile,
            referenceMap.genomeIndex,
            referenceMap.genomeDict,
            referenceMap.dbsnp,
            referenceMap.dbsnpIndex
        ])

    output:
        set val("HaplotypeCallerGVCF"), idPatient, idSample, file("${intervalBed.baseName}_${idSample}.g.vcf") into hcGenomicVCF
        set idPatient, idSample, file(intervalBed), file("${intervalBed.baseName}_${idSample}.g.vcf") into vcfsToGenotype

    when: 'haplotypecaller' in tools

    script:
    """
    gatk --java-options "-Xmx${task.memory.toGiga()}g -Xms6000m -XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10" \
        HaplotypeCaller \
        -R ${genomeFile} \
        -I ${bam} \
        -L ${intervalBed} \
        -D ${dbsnp} \
        -O ${intervalBed.baseName}_${idSample}.g.vcf \
        -ERC GVCF
    """
}

hcGenomicVCF = hcGenomicVCF.groupTuple(by:[0,1,2])

if (params.noGVCF) hcGenomicVCF.close()

process RunGenotypeGVCFs {
    tag {idSample + "-" + intervalBed.baseName}

    input:
        set idPatient, idSample, file(intervalBed), file(gvcf) from vcfsToGenotype
        set file(genomeFile), file(genomeIndex), file(genomeDict), file(dbsnp), file(dbsnpIndex) from Channel.value([
            referenceMap.genomeFile,
            referenceMap.genomeIndex,
            referenceMap.genomeDict,
            referenceMap.dbsnp,
            referenceMap.dbsnpIndex
        ])

    output:
    set val("HaplotypeCaller"), idPatient, idSample, file("${intervalBed.baseName}_${idSample}.vcf") into hcGenotypedVCF

    when: 'haplotypecaller' in tools

    script:
    // Using -L is important for speed and we have to index the interval files also
    """
    gatk --java-options -Xmx${task.memory.toGiga()}g \
        IndexFeatureFile -F ${gvcf}

    gatk --java-options -Xmx${task.memory.toGiga()}g \
        GenotypeGVCFs \
        -R ${genomeFile} \
        -L ${intervalBed} \
        -D ${dbsnp} \
        -V ${gvcf} \
        -O ${intervalBed.baseName}_${idSample}.vcf
    """
}

hcGenotypedVCF = hcGenotypedVCF.groupTuple(by:[0,1,2])

// we are merging the VCFs that are called separatelly for different intervals
// so we can have a single sorted VCF containing all the calls for a given caller

process RunSingleStrelka {
    tag {idSample}

    publishDir "${params.outdir}/VariantCalling/${idSample}/Strelka", mode: params.publishDirMode

    input:
        set idPatient, status, idSample, file(bam), file(bai) from bamsForSingleStrelka
        file(targetBED) from Channel.value(params.targetBED ? file(params.targetBED) : "null")
        set file(genomeFile), file(genomeIndex) from Channel.value([
            referenceMap.genomeFile,
            referenceMap.genomeIndex
        ])

    output:
        set val("Strelka"), idPatient, idSample, file("*.vcf.gz"), file("*.vcf.gz.tbi") into singleStrelkaOutput

    when: 'strelka' in tools

    script:
    beforeScript = params.targetBED ? "bgzip --threads ${task.cpus} -c ${targetBED} > call_targets.bed.gz ; tabix call_targets.bed.gz" : ""
    options = params.targetBED ? "--exome --callRegions call_targets.bed.gz" : ""
    """
    ${beforeScript}
    configureStrelkaGermlineWorkflow.py \
        --bam ${bam} \
        --referenceFasta ${genomeFile} \
        ${options} \
        --runDir Strelka

    python Strelka/runWorkflow.py -m local -j ${task.cpus}

    mv Strelka/results/variants/genome.*.vcf.gz \
        Strelka_${idSample}_genome.vcf.gz
    mv Strelka/results/variants/genome.*.vcf.gz.tbi \
        Strelka_${idSample}_genome.vcf.gz.tbi
    mv Strelka/results/variants/variants.vcf.gz \
        Strelka_${idSample}_variants.vcf.gz
    mv Strelka/results/variants/variants.vcf.gz.tbi \
        Strelka_${idSample}_variants.vcf.gz.tbi
  """
}

singleStrelkaOutput = singleStrelkaOutput.dump(tag:'Single Strelka')

process RunSingleManta {
    tag {idSample}

    publishDir "${params.outdir}/VariantCalling/${idSample}/Manta", mode: params.publishDirMode

    input:
        set idPatient, status, idSample, file(bam), file(bai) from bamsForSingleManta
        file(targetBED) from Channel.value(params.targetBED ? file(params.targetBED) : "null")
        set file(genomeFile), file(genomeIndex) from Channel.value([
            referenceMap.genomeFile,
            referenceMap.genomeIndex
        ])

    output:
        set val("Manta"), idPatient, idSample,  file("*.vcf.gz"), file("*.vcf.gz.tbi") into singleMantaOutput

    when: 'manta' in tools

    script:
    beforeScript = params.targetBED ? "bgzip --threads ${task.cpus} -c ${targetBED} > call_targets.bed.gz ; tabix call_targets.bed.gz" : ""
    options = params.targetBED ? "--exome --callRegions call_targets.bed.gz" : ""
    inputbam = status == 0 ? "--bam" : "--tumorBam"
    vcftype = status == 0 ? "diploid" : "tumor"
    """
    ${beforeScript}
    configManta.py \
        ${inputbam} ${bam} \
        --reference ${genomeFile} \
        ${options} \
        --runDir Manta

    python Manta/runWorkflow.py -m local -j ${task.cpus}

    mv Manta/results/variants/candidateSmallIndels.vcf.gz \
        Manta_${idSample}.candidateSmallIndels.vcf.gz
    mv Manta/results/variants/candidateSmallIndels.vcf.gz.tbi \
        Manta_${idSample}.candidateSmallIndels.vcf.gz.tbi
    mv Manta/results/variants/candidateSV.vcf.gz \
        Manta_${idSample}.candidateSV.vcf.gz
    mv Manta/results/variants/candidateSV.vcf.gz.tbi \
        Manta_${idSample}.candidateSV.vcf.gz.tbi
    mv Manta/results/variants/${vcftype}SV.vcf.gz \
        Manta_${idSample}.${vcftype}SV.vcf.gz
    mv Manta/results/variants/${vcftype}SV.vcf.gz.tbi \
        Manta_${idSample}.${vcftype}SV.vcf.gz.tbi
    """
}

singleMantaOutput = singleMantaOutput.dump(tag:'Single Manta')

/*
================================================================================
                             SOMATIC VARIANT CALLING
================================================================================
*/

// separate recalibrateBams by status
bamsNormal = Channel.create()
bamsTumor = Channel.create()

recalibratedBam
    .choice(bamsTumor, bamsNormal) {it[1] == 0 ? 1 : 0}

// Ascat, Control-FREEC, Manta Tumor-only SV
bamsForAscat = Channel.create()
bamsForMpileup = Channel.create()
bamsForSingleManta = Channel.create()

(bamsTumorTemp, bamsTumor) = bamsTumor.into(2)
(bamsNormalTemp, bamsNormal) = bamsNormal.into(2)
(bamsForAscat, bamsForMpileup, bamsForSingleManta) = bamsNormalTemp.mix(bamsTumorTemp).into(3)

// Removing status because not relevant anymore
bamsNormal = bamsNormal.map { idPatient, status, idSample, bam, bai -> [idPatient, idSample, bam, bai] }
bamsTumor = bamsTumor.map { idPatient, status, idSample, bam, bai -> [idPatient, idSample, bam, bai] }

bamsAll = bamsNormal.join(bamsTumor)

// Manta and Strelka
(bamsForManta, bamsForStrelka, bamsForStrelkaBP, bamsAll) = bamsAll.into(4)

bamsTumorNormalIntervals = bamsAll.spread(bedIntervals)
bamsForMpileup = bamsForMpileup.spread(bedIntervalsForMpileup)

// MuTect2, FreeBayes
( bamsFMT2, bamsFFB) = bamsTumorNormalIntervals.into(3)

// This will give as a list of unfiltered calls for MuTect2.
process RunMutect2 {
    tag {idSampleTumor + "_vs_" + idSampleNormal + "-" + intervalBed.baseName}

    input:
        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from bamsFMT2
        set file(genomeFile), file(genomeIndex), file(genomeDict), file(dbsnp), file(dbsnpIndex) from Channel.value([
            referenceMap.genomeFile,
            referenceMap.genomeIndex,
            referenceMap.genomeDict,
            referenceMap.dbsnp,
            referenceMap.dbsnpIndex
        ])

    output:
        set val("MuTect2"), idPatient, val("${idSampleTumor}_vs_${idSampleNormal}"), file("${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf") into mutect2Output

    when: 'mutect2' in tools

    script:
    """
    gatk --java-options "-Xmx${task.memory.toGiga()}g" \
        Mutect2 \
        -R ${genomeFile}\
        -I ${bamTumor}  -tumor ${idSampleTumor} \
        -I ${bamNormal} -normal ${idSampleNormal} \
        -L ${intervalBed} \
        -O ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf
    """
}

mutect2Output = mutect2Output.groupTuple(by:[0,1,2])

process RunFreeBayes {
    tag {idSampleTumor + "_vs_" + idSampleNormal + "-" + intervalBed.baseName}

    input:
        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(intervalBed) from bamsFFB
        file(genomeFile) from Channel.value(referenceMap.genomeFile)
        file(genomeIndex) from Channel.value(referenceMap.genomeIndex)

    output:
        set val("FreeBayes"), idPatient, val("${idSampleTumor}_vs_${idSampleNormal}"), file("${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf") into freebayesOutput

    when: 'freebayes' in tools

    script:
    """
    freebayes \
        -f ${genomeFile} \
        --pooled-continuous \
        --pooled-discrete \
        --genotype-qualities \
        --report-genotype-likelihood-max \
        --allele-balance-priors-off \
        --min-alternate-fraction 0.03 \
        --min-repeat-entropy 1 \
        --min-alternate-count 2 \
        -t ${intervalBed} \
        ${bamTumor} \
        ${bamNormal} > ${intervalBed.baseName}_${idSampleTumor}_vs_${idSampleNormal}.vcf
    """
}

freebayesOutput = freebayesOutput.groupTuple(by:[0,1,2])

vcfsToMerge = mutect2Output.mix(freebayesOutput, hcGenotypedVCF)

vcfsToMerge = vcfsToMerge.dump(tag:'VCF to merge')

process ConcatVCF {
    tag {variantCaller + "-" + idSample}

    publishDir "${params.outdir}/VariantCalling/${idSample}/${"$variantCaller"}", mode: params.publishDirMode

    input:
        set variantCaller, idPatient, idSample, file(vcFiles) from vcfsToMerge
        file(genomeIndex) from Channel.value(referenceMap.genomeIndex)
        file(targetBED) from Channel.value(params.targetBED ? file(params.targetBED) : "null")

    output:
    // we have this funny *_* pattern to avoid copying the raw calls to publishdir
        set variantCaller, idPatient, idSample, file("*_*.vcf.gz"), file("*_*.vcf.gz.tbi") into vcfConcatenated

    when: ('haplotypecaller' in tools || 'mutect2' in tools || 'freebayes' in tools)

    script:
    if (variantCaller == 'HaplotypeCallerGVCF') outputFile = "HaplotypeCaller_${idSample}.g.vcf"
    else outputFile = "${variantCaller}_${idSample}.vcf"
    options = params.targetBED ? "-t ${targetBED}" : ""
    """
    concatenateVCFs.sh -i ${genomeIndex} -c ${task.cpus} -o ${outputFile} ${options}
    """
}

vcfConcatenated = vcfConcatenated.dump(tag:'VCF')

process RunStrelka {
    tag {idSampleTumor + "_vs_" + idSampleNormal}

    publishDir "${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Strelka", mode: params.publishDirMode

    input:
        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor) from bamsForStrelka
        file(targetBED) from Channel.value(params.targetBED ? file(params.targetBED) : "null")
        set file(genomeFile), file(genomeIndex), file(genomeDict) from Channel.value([
            referenceMap.genomeFile,
            referenceMap.genomeIndex,
            referenceMap.genomeDict
        ])

    output:
        set val("Strelka"), idPatient, val("${idSampleTumor}_vs_${idSampleNormal}"), file("*.vcf.gz"), file("*.vcf.gz.tbi") into strelkaOutput

    when: 'strelka' in tools

    script:
    beforeScript = params.targetBED ? "bgzip --threads ${task.cpus} -c ${targetBED} > call_targets.bed.gz ; tabix call_targets.bed.gz" : ""
    options = params.targetBED ? "--exome --callRegions call_targets.bed.gz" : ""
    """
    ${beforeScript}
    configureStrelkaSomaticWorkflow.py \
        --tumor ${bamTumor} \
        --normal ${bamNormal} \
        --referenceFasta ${genomeFile} \
        ${options} \
        --runDir Strelka

    python Strelka/runWorkflow.py -m local -j ${task.cpus}

    mv Strelka/results/variants/somatic.indels.vcf.gz \
        Strelka_${idSampleTumor}_vs_${idSampleNormal}_somatic_indels.vcf.gz
    mv Strelka/results/variants/somatic.indels.vcf.gz.tbi \
        Strelka_${idSampleTumor}_vs_${idSampleNormal}_somatic_indels.vcf.gz.tbi
    mv Strelka/results/variants/somatic.snvs.vcf.gz \
        Strelka_${idSampleTumor}_vs_${idSampleNormal}_somatic_snvs.vcf.gz
    mv Strelka/results/variants/somatic.snvs.vcf.gz.tbi \
        Strelka_${idSampleTumor}_vs_${idSampleNormal}_somatic_snvs.vcf.gz.tbi
  """
}

strelkaOutput = strelkaOutput.dump(tag:'Strelka')
(strelkaIndels, strelkaSNVS) = strelkaOutput.into(2)

process RunManta {
    tag {idSampleTumor + "_vs_" + idSampleNormal}

    publishDir "${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Manta", mode: params.publishDirMode

    input:
        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor) from bamsForManta
        file(targetBED) from Channel.value(params.targetBED ? file(params.targetBED) : "null")
        set file(genomeFile), file(genomeIndex) from Channel.value([
            referenceMap.genomeFile,
            referenceMap.genomeIndex
        ])

    output:
        set val("Manta"), idPatient, val("${idSampleTumor}_vs_${idSampleNormal}"), file("*.vcf.gz"), file("*.vcf.gz.tbi") into mantaOutput
        set idPatient, idSampleNormal, idSampleTumor, file("*.candidateSmallIndels.vcf.gz"), file("*.candidateSmallIndels.vcf.gz.tbi") into mantaToStrelka

    when: 'manta' in tools

    script:
    beforeScript = params.targetBED ? "bgzip --threads ${task.cpus} -c ${targetBED} > call_targets.bed.gz ; tabix call_targets.bed.gz" : ""
    options = params.targetBED ? "--exome --callRegions call_targets.bed.gz" : ""
    """
    ${beforeScript}
        configManta.py \
        --normalBam ${bamNormal} \
        --tumorBam ${bamTumor} \
        --reference ${genomeFile} \
        ${options} \
        --runDir Manta

    python Manta/runWorkflow.py -m local -j ${task.cpus}

    mv Manta/results/variants/candidateSmallIndels.vcf.gz \
        Manta_${idSampleTumor}_vs_${idSampleNormal}.candidateSmallIndels.vcf.gz
    mv Manta/results/variants/candidateSmallIndels.vcf.gz.tbi \
        Manta_${idSampleTumor}_vs_${idSampleNormal}.candidateSmallIndels.vcf.gz.tbi
    mv Manta/results/variants/candidateSV.vcf.gz \
        Manta_${idSampleTumor}_vs_${idSampleNormal}.candidateSV.vcf.gz
    mv Manta/results/variants/candidateSV.vcf.gz.tbi \
        Manta_${idSampleTumor}_vs_${idSampleNormal}.candidateSV.vcf.gz.tbi
    mv Manta/results/variants/diploidSV.vcf.gz \
        Manta_${idSampleTumor}_vs_${idSampleNormal}.diploidSV.vcf.gz
    mv Manta/results/variants/diploidSV.vcf.gz.tbi \
        Manta_${idSampleTumor}_vs_${idSampleNormal}.diploidSV.vcf.gz.tbi
    mv Manta/results/variants/somaticSV.vcf.gz \
        Manta_${idSampleTumor}_vs_${idSampleNormal}.somaticSV.vcf.gz
    mv Manta/results/variants/somaticSV.vcf.gz.tbi \
        Manta_${idSampleTumor}_vs_${idSampleNormal}.somaticSV.vcf.gz.tbi
    """
}

mantaOutput = mantaOutput.dump(tag:'Manta')
(mantaSomaticSV, mantaDiploidSV) = mantaOutput.into(2)

bamsForStrelkaBP = bamsForStrelkaBP.map {
    idPatientNormal, idSampleNormal, bamNormal, baiNormal, idSampleTumor, bamTumor, baiTumor ->
    [idPatientNormal, idSampleNormal, idSampleTumor, bamNormal, baiNormal, bamTumor, baiTumor]
}.join(mantaToStrelka, by:[0,1,2]).map {
    idPatientNormal, idSampleNormal, idSampleTumor, bamNormal, baiNormal, bamTumor, baiTumor, mantaCSI, mantaCSIi ->
    [idPatientNormal, idSampleNormal, bamNormal, baiNormal, idSampleTumor, bamTumor, baiTumor, mantaCSI, mantaCSIi]
}

process RunStrelkaBP {
    tag {idSampleTumor + "_vs_" + idSampleNormal}

    publishDir "${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/Strelka", mode: params.publishDirMode

    input:
        set idPatient, idSampleNormal, file(bamNormal), file(baiNormal), idSampleTumor, file(bamTumor), file(baiTumor), file(mantaCSI), file(mantaCSIi) from bamsForStrelkaBP
        file(targetBED) from Channel.value(params.targetBED ? file(params.targetBED) : "null")
        set file(genomeFile), file(genomeIndex), file(genomeDict) from Channel.value([
            referenceMap.genomeFile,
            referenceMap.genomeIndex,
            referenceMap.genomeDict
        ])

    output:
        set val("Strelka"), idPatient, val("${idSampleTumor}_vs_${idSampleNormal}"), file("*.vcf.gz"), file("*.vcf.gz.tbi") into strelkaBPOutput

    when: 'strelka' in tools && 'manta' in tools && params.strelkaBP

    script:
    beforeScript = params.targetBED ? "bgzip --threads ${task.cpus} -c ${targetBED} > call_targets.bed.gz ; tabix call_targets.bed.gz" : ""
    options = params.targetBED ? "--exome --callRegions call_targets.bed.gz" : ""
    """
    ${beforeScript}
    configureStrelkaSomaticWorkflow.py \
        --tumor ${bamTumor} \
        --normal ${bamNormal} \
        --referenceFasta ${genomeFile} \
        --indelCandidates ${mantaCSI} \
        ${options} \
        --runDir Strelka

    python Strelka/runWorkflow.py -m local -j ${task.cpus}

    mv Strelka/results/variants/somatic.indels.vcf.gz \
        StrelkaBP_${idSampleTumor}_vs_${idSampleNormal}_somatic_indels.vcf.gz
    mv Strelka/results/variants/somatic.indels.vcf.gz.tbi \
        StrelkaBP_${idSampleTumor}_vs_${idSampleNormal}_somatic_indels.vcf.gz.tbi
    mv Strelka/results/variants/somatic.snvs.vcf.gz \
        StrelkaBP_${idSampleTumor}_vs_${idSampleNormal}_somatic_snvs.vcf.gz
    mv Strelka/results/variants/somatic.snvs.vcf.gz.tbi \
        StrelkaBP_${idSampleTumor}_vs_${idSampleNormal}_somatic_snvs.vcf.gz.tbi
  """
}

strelkaBPOutput = strelkaBPOutput.dump(tag:'Strelka BP')

// Run commands and code from Malin Larsson
// Based on Jesper Eisfeldt's code
process RunAlleleCount {
    tag {idSample}

    input:
        set idPatient, status, idSample, file(bam), file(bai) from bamsForAscat
        set file(acLoci), file(genomeFile), file(genomeIndex), file(genomeDict) from Channel.value([
            referenceMap.acLoci,
            referenceMap.genomeFile,
            referenceMap.genomeIndex,
            referenceMap.genomeDict
        ])

    output:
        set idPatient, status, idSample, file("${idSample}.alleleCount") into alleleCountOutput

    when: 'ascat' in tools

    script:
    """
    alleleCounter \
        -l ${acLoci} \
        -r ${genomeFile} \
        -b ${bam} \
        -o ${idSample}.alleleCount;
    """
}

alleleCountNormal = Channel.create()
alleleCountTumor = Channel.create()

alleleCountOutput
    .choice(alleleCountTumor, alleleCountNormal) {it[1] == 0 ? 1 : 0}

alleleCountOutput = alleleCountNormal.combine(alleleCountTumor)

alleleCountOutput = alleleCountOutput.map {
    idPatientNormal, statusNormal, idSampleNormal, alleleCountNormal,
    idPatientTumor,  statusTumor,  idSampleTumor,  alleleCountTumor ->
    [idPatientNormal, idSampleNormal, idSampleTumor, alleleCountNormal, alleleCountTumor]
}

// R script from Malin Larssons bitbucket repo:
// https://bitbucket.org/malinlarsson/somatic_wgs_pipeline
process RunConvertAlleleCounts {
    tag {idSampleTumor + "_vs_" + idSampleNormal}

    publishDir "${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/ASCAT", mode: params.publishDirMode

    input:
        set idPatient, idSampleNormal, idSampleTumor, file(alleleCountNormal), file(alleleCountTumor) from alleleCountOutput

    output:
        set idPatient, idSampleNormal, idSampleTumor, file("${idSampleNormal}.BAF"), file("${idSampleNormal}.LogR"), file("${idSampleTumor}.BAF"), file("${idSampleTumor}.LogR") into convertAlleleCountsOutput

    when: 'ascat' in tools

    script:
    gender = patientGenders[idPatient]
    """
    convertAlleleCounts.r ${idSampleTumor} ${alleleCountTumor} ${idSampleNormal} ${alleleCountNormal} ${gender}
    """
}

// R scripts from Malin Larssons bitbucket repo:
// https://bitbucket.org/malinlarsson/somatic_wgs_pipeline
process RunAscat {
    tag {idSampleTumor + "_vs_" + idSampleNormal}

    publishDir "${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/ASCAT", mode: params.publishDirMode

    input:
        set idPatient, idSampleNormal, idSampleTumor, file(bafNormal), file(logrNormal), file(bafTumor), file(logrTumor) from convertAlleleCountsOutput
        file(acLociGC) from Channel.value([referenceMap.acLociGC])

    output:
        set val("ASCAT"), idPatient, idSampleNormal, idSampleTumor, file("${idSampleTumor}.*.{png,txt}") into ascatOutput

    when: 'ascat' in tools

    script:
    """
    # get rid of "chr" string if there is any
    for f in *BAF *LogR; do sed 's/chr//g' \$f > tmpFile; mv tmpFile \$f;done
    run_ascat.r ${bafTumor} ${logrTumor} ${bafNormal} ${logrNormal} ${idSampleTumor} ${baseDir} ${acLociGC}
    """
}

ascatOutput.dump(tag:'ASCAT')

process RunMpileup {
    tag {idSample + "-" + intervalBed.baseName}

    input:
        set idPatient, status, idSample, file(bam), file(bai), file(intervalBed) from bamsForMpileup
        set file(genomeFile), file(genomeIndex) from Channel.value([
            referenceMap.genomeFile,
            referenceMap.genomeIndex
        ])

    output:
        set idPatient, status, idSample, file("${idSample}_${intervalBed.baseName}.pileup.gz") into mpileupToMerge

    when: ('controlfreec' in tools || 'mpileup' in tools)

    script:
    """
    samtools mpileup \
        -f ${genomeFile} ${bam} \
        -l ${intervalBed} \
    | bgzip --threads ${task.cpus} -c > ${idSample}_${intervalBed.baseName}.pileup.gz
    """
}

mpileupToMerge = mpileupToMerge.groupTuple(by:[0,1,2])

process MergeMpileup {
    tag {idSample}

    publishDir params.outdir, mode: params.publishDirMode, saveAs: { it == "${idSample}.pileup.gz" ? "VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/mpileup/${it}" : '' }

    input:
        set idPatient, status, idSample, file(mpileup) from mpileupToMerge

    output:
        set idPatient, status, idSample, file("${idSample}.pileup.gz") into mpileupOutput

    when: ('controlfreec' in tools || 'mpileup' in tools)

    script:
    """
    for i in `ls -1v *.pileup.gz`;
        do zcat \$i >> ${idSample}.pileup
    done

    bgzip --threads ${task.cpus} -c ${idSample}.pileup > ${idSample}.pileup.gz

    rm ${idSample}.pileup
    """
}

mpileupOutput = mpileupOutput.dump(tag:'mpileup')

mpileupNormal = Channel.create()
mpileupTumor = Channel.create()

mpileupOutput
    .choice(mpileupTumor, mpileupNormal) {it[1] == 0 ? 1 : 0}

mpileupOutput = mpileupNormal.combine(mpileupTumor)

mpileupOutput = mpileupOutput.map {
    idPatientNormal, statusNormal, idSampleNormal, mpileupNormal,
    idPatientTumor,  statusTumor,  idSampleTumor,  mpileupTumor ->
    [idPatientNormal, idSampleNormal, idSampleTumor, mpileupNormal, mpileupTumor]
}

process RunControlFreec {
    tag {idSampleTumor + "_vs_" + idSampleNormal}

    publishDir "${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/controlFREEC", mode: params.publishDirMode

    input:
        set idPatient, idSampleNormal, idSampleTumor, file(mpileupNormal), file(mpileupTumor) from mpileupOutput
        set file(genomeFile), file(genomeIndex), file(dbsnp), file(dbsnpIndex), file(chrDir), file(chrLength) from Channel.value([
            referenceMap.genomeFile,
            referenceMap.genomeIndex,
            referenceMap.dbsnp,
            referenceMap.dbsnpIndex,
            referenceMap.chrDir,
            referenceMap.chrLength
        ])

    output:
        set idPatient, idSampleNormal, idSampleTumor, file("${idSampleTumor}.pileup.gz_CNVs"), file("${idSampleTumor}.pileup.gz_ratio.txt"), file("${idSampleTumor}.pileup.gz_normal_CNVs"), file("${idSampleTumor}.pileup.gz_normal_ratio.txt"), file("${idSampleTumor}.pileup.gz_BAF.txt"), file("${idSampleNormal}.pileup.gz_BAF.txt") into controlFreecOutputVisualization
        set file("*.pileup.gz*"), file("${idSampleTumor}_vs_${idSampleNormal}.config.txt") into controlFreecOutput

    when: 'controlfreec' in tools

    script:
    config = "${idSampleTumor}_vs_${idSampleNormal}.config.txt"
    gender = patientGenders[idPatient]
    """
    touch ${config}
    echo "[general]" >> ${config}
    echo "BedGraphOutput = TRUE" >> ${config}
    echo "chrFiles = \${PWD}/${referenceMap.chrDir.fileName}" >> ${config}
    echo "chrLenFile = \${PWD}/${referenceMap.chrLength.fileName}" >> ${config}
    echo "coefficientOfVariation = 0.05" >> ${config}
    echo "contaminationAdjustment = TRUE" >> ${config}
    echo "forceGCcontentNormalization = 0" >> ${config}
    echo "maxThreads = ${task.cpus}" >> ${config}
    echo "minimalSubclonePresence = 20" >> ${config}
    echo "ploidy = 2,3,4" >> ${config}
    echo "sex = ${gender}" >> ${config}
    echo "window = 50000" >> ${config}
    echo "" >> ${config}

    echo "[control]" >> ${config}
    echo "inputFormat = pileup" >> ${config}
    echo "mateFile = \${PWD}/${mpileupNormal}" >> ${config}
    echo "mateOrientation = FR" >> ${config}
    echo "" >> ${config}

    echo "[sample]" >> ${config}
    echo "inputFormat = pileup" >> ${config}
    echo "mateFile = \${PWD}/${mpileupTumor}" >> ${config}
    echo "mateOrientation = FR" >> ${config}
    echo "" >> ${config}

    echo "[BAF]" >> ${config}
    echo "SNPfile = ${referenceMap.dbsnp.fileName}" >> ${config}

    freec -conf ${config}
    """
}

process RunControlFreecVisualization {

    tag {idSampleTumor + "_vs_" + idSampleNormal}

    publishDir "${params.outdir}/VariantCalling/${idSampleTumor}_vs_${idSampleNormal}/controlFREEC", mode: params.publishDirMode

    input:
    set idPatient, idSampleNormal, idSampleTumor, file(cnvTumor), file(ratioTumor), file(cnvNormal), file(ratioNormal), file(bafTumor), file(bafNormal) from controlFreecOutputVisualization

    output:
    set file("*.txt"), file("*.png"), file("*.bed") into controlFreecOutputFinal

    when: 'controlfreec' in tools

    """
    cat /opt/conda/envs/sarek-2.5dev/bin/assess_significance.R | R --slave --args ${cnvTumor} ${ratioTumor}
    cat /opt/conda/envs/sarek-2.5dev/bin/assess_significance.R | R --slave --args ${cnvNormal} ${ratioNormal}
    cat /opt/conda/envs/sarek-2.5dev/bin/makeGraph.R | R --slave --args 2 ${ratioTumor} ${bafTumor}
    cat /opt/conda/envs/sarek-2.5dev/bin/makeGraph.R | R --slave --args 2 ${ratioNormal} ${bafNormal}
    perl /opt/conda/envs/sarek-2.5dev/bin/freec2bed.pl -f ${ratioTumor} > ${idSampleTumor}.bed
    perl /opt/conda/envs/sarek-2.5dev/bin/freec2bed.pl -f ${ratioNormal} > ${idSampleNormal}.bed
    """
}

vcfToKeep = Channel.empty().mix(
    vcfConcatenated.map {
        variantcaller, idPatient, idSample, vcf, tbi ->
        [variantcaller, idSample, vcf]
    },
    singleStrelkaOutput.map {
        variantcaller, idPatient, idSample, vcf, tbi ->
        [variantcaller, idSample, vcf[1]]
    },
    singleMantaOutput.map {
        variantcaller, idPatient, idSample, vcf, tbi ->
        [variantcaller, idSample, vcf[2]]
    },
    mantaDiploidSV.map {
        variantcaller, idPatient, idSample, vcf, tbi ->
        [variantcaller, idSample, vcf[2]]
    },
    mantaSomaticSV.map {
        variantcaller, idPatient, idSample, vcf, tbi ->
        [variantcaller, idSample, vcf[3]]
    },
    strelkaIndels.map {
        variantcaller, idPatient, idSample, vcf, tbi ->
        [variantcaller, idSample, vcf[0]]
    },
    strelkaSNVS.map {
        variantcaller, idPatient, idSample, vcf, tbi ->
        [variantcaller, idSample, vcf[1]]
    })

(vcfForBCFtools, vcfForVCFtools, vcfForAnnotation) = vcfToKeep.into(3)

process RunBcftoolsStats {
    tag {"${variantCaller} - ${vcf}"}

    publishDir "${params.outdir}/Reports/${idSample}/BCFToolsStats", mode: params.publishDirMode

    input:
        set variantCaller, idSample, file(vcf) from vcfForBCFtools

    output:
        file ("*.bcf.tools.stats.out") into bcfReport

    when: !params.noReports

    script:
    """
    bcftools stats ${vcf} > ${reduceVCF(vcf)}.bcf.tools.stats.out
    """
}

bcfReport = bcfReport.dump(tag:'BCFTools')

process RunVcftools {
    tag {"${variantCaller} - ${vcf}"}

    publishDir "${params.outdir}/Reports/${idSample}/VCFTools", mode: params.publishDirMode

    input:
        set variantCaller, idSample, file(vcf) from vcfForVCFtools

    output:
        file ("${reduceVCF(vcf)}.*") into vcfReport

    when: !params.noReports

    script:
    """
    vcftools \
    --gzvcf ${vcf} \
    --relatedness2 \
    --out ${reduceVCF(vcf)}

    vcftools \
    --gzvcf ${vcf} \
    --TsTv-by-count \
    --out ${reduceVCF(vcf)}

    vcftools \
    --gzvcf ${vcf} \
    --TsTv-by-qual \
    --out ${reduceVCF(vcf)}

    vcftools \
    --gzvcf ${vcf} \
    --FILTER-summary \
    --out ${reduceVCF(vcf)}
    """
}

vcfReport = vcfReport.dump(tag:'VCFTools')

/*
================================================================================
                                   ANNOTATION
================================================================================
*/

vcfToAnnotate = Channel.create()

if (step == 'annotate') {
    vcfNotToAnnotate = Channel.create()

    if (annotateVCF == []) {
    // Sarek, by default, annotates all available vcfs that it can find in the VariantCalling directory
    // Excluding vcfs from FreeBayes, and g.vcf from HaplotypeCaller
    // Basically it's: VariantCalling/*/{HaplotypeCaller,Manta,MuTect2,Strelka}/*.vcf.gz
    // Without *SmallIndels.vcf.gz from Manta, and *.genome.vcf.gz from Strelka
    // The small snippet `vcf.minus(vcf.fileName)[-2]` catches idSample
    // This field is used to output final annotated VCFs in the correct directory
      Channel.empty().mix(
        Channel.fromPath("${params.outdir}/VariantCalling/*/HaplotypeCaller/*.vcf.gz")
          .flatten().map{vcf -> ['haplotypecaller', vcf.minus(vcf.fileName)[-2].toString(), vcf]},
        Channel.fromPath("${params.outdir}/VariantCalling/*/Manta/*[!candidate]SV.vcf.gz")
          .flatten().map{vcf -> ['manta', vcf.minus(vcf.fileName)[-2].toString(), vcf]},
        Channel.fromPath("${params.outdir}/VariantCalling/*/MuTect2/*.vcf.gz")
          .flatten().map{vcf -> ['mutect2', vcf.minus(vcf.fileName)[-2].toString(), vcf]},
        Channel.fromPath("${params.outdir}/VariantCalling/*/Strelka/*{somatic,variant}*.vcf.gz")
          .flatten().map{vcf -> ['strelka', vcf.minus(vcf.fileName)[-2].toString(), vcf]},
      ).choice(vcfToAnnotate, vcfNotToAnnotate) {
        annotateTools == [] || (annotateTools != [] && it[0] in annotateTools) ? 0 : 1
      }
    } else if (annotateTools == []) {
    // Annotate user-submitted VCFs
    // If user-submitted, Sarek assume that the idSample should be assumed automatically
      vcfToAnnotate = Channel.fromPath(annotateVCF)
        .map{vcf -> ['userspecified', vcf.minus(vcf.fileName)[-2].toString(), vcf]}
    } else exit 1, "specify only tools or files to annotate, not both"

    vcfNotToAnnotate.close()
    vcfForAnnotation = vcfForAnnotation.mix(vcfToAnnotate)
}

// as now have the list of VCFs to annotate, the first step is to annotate with allele frequencies, if there are any

(vcfForSnpeff, vcfForVep) = vcfForAnnotation.into(2)

vcfForVep = vcfForVep.map {
  variantCaller, idSample, vcf ->
  ["VEP", variantCaller, idSample, vcf, null]
}

process RunSnpeff {
  tag {"${idSample} - ${variantCaller} - ${vcf}"}

  publishDir params.outdir, mode: params.publishDirMode, saveAs: {
    if (it == "${reducedVCF}_snpEff.ann.vcf") null
    else "Annotation/${idSample}/snpEff/${it}"
  }

  input:
    set variantCaller, idSample, file(vcf) from vcfForSnpeff
    file dataDir from Channel.value(params.snpEff_cache ? file(params.snpEff_cache) : "null")
    val snpeffDb from Channel.value(params.genomes[params.genome].snpeffDb)

  output:
    set file("${reducedVCF}_snpEff.genes.txt"), file("${reducedVCF}_snpEff.csv"), file("${reducedVCF}_snpEff.summary.html") into snpeffOutput
    set val("snpEff"), variantCaller, idSample, file("${reducedVCF}_snpEff.ann.vcf") into snpeffVCF

  when: 'snpeff' in tools || 'merge' in tools

  script:
  reducedVCF = reduceVCF(vcf)
  cache = (params.snpEff_cache && params.annotation_cache) ? "-dataDir \${PWD}/${dataDir}" : ""
  """
  echo ${task.container}

  snpEff -Xmx${task.memory.toGiga()}g \
  ${snpeffDb} \
  -csvStats ${reducedVCF}_snpEff.csv \
  -nodownload \
  ${cache} \
  -canon \
  -v \
  ${vcf} \
  > ${reducedVCF}_snpEff.ann.vcf

  mv snpEff_summary.html ${reducedVCF}_snpEff.summary.html
  """
}

snpeffOutput = snpeffOutput.dump(tag:'snpEff')

if ('merge' in tools) {
  // When running in the 'merge' mode
  // snpEff output is used as VEP input
  // Used a feedback loop from vcfCompressed
  // https://github.com/nextflow-io/patterns/tree/master/feedback-loop

  vcfCompressed = Channel.create()

  vcfForVep = Channel.empty().mix(
    vcfCompressed.until({ it[0]=="merge" })
  )
}

process RunVEP {
  tag {"${idSample} - ${variantCaller} - ${vcf}"}

  publishDir params.outdir, mode: params.publishDirMode, saveAs: {
    if (it == "${reducedVCF}_VEP.summary.html") "Annotation/${idSample}/VEP/${it}"
    else null
  }

  input:
    set annotator, variantCaller,  idSample, file(vcf), file(idx) from vcfForVep
    file dataDir from Channel.value(params.vep_cache ? file(params.vep_cache) : "null")
    val cache_version from Channel.value(params.genomes[params.genome].vepCacheVersion)
    set file(cadd_WG_SNVs), file(cadd_WG_SNVs_tbi), file(cadd_InDels), file(cadd_InDels_tbi) from Channel.value([
      params.cadd_WG_SNVs ? file(params.cadd_WG_SNVs) : "null",
      params.cadd_WG_SNVs_tbi ? file(params.cadd_WG_SNVs_tbi) : "null",
      params.cadd_InDels ? file(params.cadd_InDels) : "null",
      params.cadd_InDels_tbi ? file(params.cadd_InDels_tbi) : "null"
    ])

  output:
    set finalAnnotator, variantCaller, idSample, file("${reducedVCF}_VEP.ann.vcf") into vepVCF
    file("${reducedVCF}_VEP.summary.html") into vepReport

  when: 'vep' in tools || 'merge' in tools

  script:
  reducedVCF = reduceVCF(vcf)
  finalAnnotator = annotator == "snpEff" ? 'merge' : 'VEP'
  genome = params.genome == 'smallGRCh37' ? 'GRCh37' : params.genome
  dir_cache = (params.vep_cache && params.annotation_cache) ? " \${PWD}/${dataDir}" : "/.vep"
  cadd = (params.cadd_cache && params.cadd_WG_SNVs && params.cadd_InDels) ? "--plugin CADD,whole_genome_SNVs.tsv.gz,InDels.tsv.gz" : ""
  genesplicer = params.genesplicer ? "--plugin GeneSplicer,/opt/conda/envs/sarek-2.3/bin/genesplicer,/opt/conda/envs/sarek-2.3/share/genesplicer-1.0-1/human,context=200,tmpdir=\$PWD/${reducedVCF}" : "--offline"
  """
  mkdir ${reducedVCF}

  vep \
  -i ${vcf} \
  -o ${reducedVCF}_VEP.ann.vcf \
  --assembly ${genome} \
  ${cadd} \
  ${genesplicer} \
  --cache \
  --cache_version ${cache_version} \
  --dir_cache ${dir_cache} \
  --everything \
  --filter_common \
  --fork ${task.cpus} \
  --format vcf \
  --per_gene \
  --stats_file ${reducedVCF}_VEP.summary.html \
  --total_length \
  --vcf

  rm -rf ${reducedVCF}
  """
}

vepReport = vepReport.dump(tag:'VEP')

vcfToCompress = snpeffVCF.mix(vepVCF)

process CompressVCF {
  tag {"${idSample} - ${annotator} - ${vcf}"}

  publishDir "${params.outdir}/Annotation/${idSample}/${finalAnnotator}", mode: params.publishDirMode

  input:
    set annotator, variantCaller, idSample, file(vcf) from vcfToCompress

  output:
    set annotator, variantCaller, idSample, file("*.vcf.gz"), file("*.vcf.gz.tbi") into (vcfCompressed, vcfCompressedoutput)

  script:
  reducedVCF = reduceVCF(vcf)
  finalAnnotator = annotator == "merge" ? "VEP" : annotator
  """
  bgzip < ${vcf} > ${vcf}.gz
  tabix ${vcf}.gz
  """
}

vcfCompressedoutput = vcfCompressedoutput.dump(tag:'VCF')

/*
================================================================================
                                     MultiQC
================================================================================
*/

reportsForMultiQC = Channel.empty()
    .mix(
        bamQCmappedReport,
        bamQCrecalibratedReport,
        bcfReport,
        fastQCreport,
        markDuplicatesReport,
        samtoolsStatsReport,
        snpeffOutput,
        vcfReport
    ).collect()

process RunMultiQC {
    publishDir "${params.outdir}/Reports/MultiQC", mode: params.publishDirMode

    input:
        file (multiqcConfig) from createMultiQCconfig()
        file (reports) from reportsForMultiQC
        file (versions) from software_versions_yaml

    output:
        set file("*multiqc_report.html"), file("*multiqc_data") into multiQCReport

    when: !params.noReports

    script:
    """
    multiqc -f -v .
    """
}

multiQCReport.dump(tag:'MultiQC')

/*
 * Completion e-mail notification
 */
workflow.onComplete {

    // Set up the e-mail variables
    def subject = "[nf-core/sarek] Successful: $workflow.runName"
    if (!workflow.success){
        subject = "[nf-core/sarek] FAILED: $workflow.runName"
    }
    def email_fields = [:]
    email_fields['version'] = workflow.manifest.version
    email_fields['runName'] = custom_runName ?: workflow.runName
    email_fields['success'] = workflow.success
    email_fields['dateComplete'] = workflow.complete
    email_fields['duration'] = workflow.duration
    email_fields['exitStatus'] = workflow.exitStatus
    email_fields['errorMessage'] = (workflow.errorMessage ?: 'None')
    email_fields['errorReport'] = (workflow.errorReport ?: 'None')
    email_fields['commandLine'] = workflow.commandLine
    email_fields['projectDir'] = workflow.projectDir
    email_fields['summary'] = summary
    email_fields['summary']['Date Started'] = workflow.start
    email_fields['summary']['Date Completed'] = workflow.complete
    email_fields['summary']['Pipeline script file path'] = workflow.scriptFile
    email_fields['summary']['Pipeline script hash ID'] = workflow.scriptId
    if (workflow.repository) email_fields['summary']['Pipeline repository Git URL'] = workflow.repository
    if (workflow.commitId) email_fields['summary']['Pipeline repository Git Commit'] = workflow.commitId
    if (workflow.revision) email_fields['summary']['Pipeline Git branch/tag'] = workflow.revision
    if (workflow.container) email_fields['summary']['Docker image'] = workflow.container
    email_fields['summary']['Nextflow Version'] = workflow.nextflow.version
    email_fields['summary']['Nextflow Build'] = workflow.nextflow.build
    email_fields['summary']['Nextflow Compile Timestamp'] = workflow.nextflow.timestamp

    // TODO nf-core: If not using MultiQC, strip out this code (including params.maxMultiqcEmailFileSize)
    // On success try attach the multiqc report
    def mqc_report = null
    try {
        if (workflow.success) {
            mqc_report = multiqc_report.getVal()
            if (mqc_report.getClass() == ArrayList){
                log.warn "[nf-core/sarek] Found multiple reports from process 'multiqc', will use only one"
                mqc_report = mqc_report[0]
            }
        }
    } catch (all) {
        log.warn "[nf-core/sarek] Could not attach MultiQC report to summary email"
    }

    // Render the TXT template
    def engine = new groovy.text.GStringTemplateEngine()
    def tf = new File("$baseDir/assets/email_template.txt")
    def txt_template = engine.createTemplate(tf).make(email_fields)
    def email_txt = txt_template.toString()

    // Render the HTML template
    def hf = new File("$baseDir/assets/email_template.html")
    def html_template = engine.createTemplate(hf).make(email_fields)
    def email_html = html_template.toString()

    // Render the sendmail template
    def smail_fields = [ email: params.email, subject: subject, email_txt: email_txt, email_html: email_html, baseDir: "$baseDir", mqcFile: mqc_report, mqcMaxSize: params.maxMultiqcEmailFileSize.toBytes() ]
    def sf = new File("$baseDir/assets/sendmail_template.txt")
    def sendmail_template = engine.createTemplate(sf).make(smail_fields)
    def sendmail_html = sendmail_template.toString()

    // Send the HTML e-mail
    if (params.email) {
        try {
            if ( params.plaintext_email ){ throw GroovyException('Send plaintext e-mail, not HTML') }
            // Try to send HTML e-mail using sendmail
            [ 'sendmail', '-t' ].execute() << sendmail_html
            log.info "[nf-core/sarek] Sent summary e-mail to $params.email (sendmail)"
        } catch (all) {
            // Catch failures and try with plaintext
            [ 'mail', '-s', subject, params.email ].execute() << email_txt
            log.info "[nf-core/sarek] Sent summary e-mail to $params.email (mail)"
        }
    }

    // Write summary e-mail HTML to a file
    def output_d = new File( "${params.outdir}/pipeline_info/" )
    if ( !output_d.exists() ) output_d.mkdirs()
    def output_hf = new File( output_d, "pipeline_report.html" )
    output_hf.withWriter { w -> w << email_html }
    def output_tf = new File( output_d, "pipeline_report.txt" )
    output_tf.withWriter { w -> w << email_txt }

    c_reset = params.monochrome_logs ? '' : "\033[0m";
    c_purple = params.monochrome_logs ? '' : "\033[0;35m";
    c_green = params.monochrome_logs ? '' : "\033[0;32m";
    c_red = params.monochrome_logs ? '' : "\033[0;31m";

    if (workflow.stats.ignoredCountFmt > 0 && workflow.success) {
        log.info "${c_purple}Warning, pipeline completed, but with errored process(es) ${c_reset}"
        log.info "${c_red}Number of ignored errored process(es) : ${workflow.stats.ignoredCountFmt} ${c_reset}"
        log.info "${c_green}Number of successfully ran process(es) : ${workflow.stats.succeedCountFmt} ${c_reset}"
    }

    if (workflow.success) log.info "${c_purple}[nf-core/sarek]${c_green} Pipeline completed successfully${c_reset}"
    else {
        checkHostname()
        log.info "${c_purple}[nf-core/sarek]${c_red} Pipeline completed with errors${c_reset}"
    }
}

def nfcoreHeader(){
    // Log colors ANSI codes
    c_reset  = params.monochrome_logs ? '' : "\033[0m";
    c_dim    = params.monochrome_logs ? '' : "\033[2m";
    c_black  = params.monochrome_logs ? '' : "\033[0;30m";
    c_red    = params.monochrome_logs ? '' : "\033[0;31m";
    c_green  = params.monochrome_logs ? '' : "\033[0;32m";
    c_yellow = params.monochrome_logs ? '' : "\033[0;33m";
    c_blue   = params.monochrome_logs ? '' : "\033[0;34m";
    c_purple = params.monochrome_logs ? '' : "\033[0;35m";
    c_cyan   = params.monochrome_logs ? '' : "\033[0;36m";
    c_white  = params.monochrome_logs ? '' : "\033[0;37m";

    return """    ${c_dim}----------------------------------------------------${c_reset}
                                            ${c_green},--.${c_black}/${c_green},-.${c_reset}
    ${c_blue}        ___     __   __   __   ___     ${c_green}/,-._.--~\'${c_reset}
    ${c_blue}  |\\ | |__  __ /  ` /  \\ |__) |__         ${c_yellow}}  {${c_reset}
    ${c_blue}  | \\| |       \\__, \\__/ |  \\ |___     ${c_green}\\`-._,-`-,${c_reset}
                                            ${c_green}`._,._,\'${c_reset}
    ${c_black}       ____      ${c_blue}  _____               _ ${c_reset}
    ${c_black}     .' ${c_green}_${c_black}  `.    ${c_blue} / ____|             | | ${c_reset}
    ${c_black}    /  ${c_green}|\\${c_white}`-_${c_black} \\ ${c_blue}  | (___  ___  _ __ __ | | __ ${c_reset}
    ${c_black}   |   ${c_green}| \\  ${c_white}`-${c_black}| ${c_blue}  \\___ \\/__ \\| __/ _\\| |/ / ${c_reset}
    ${c_black}    \\ ${c_green}|   \\  ${c_black}/ ${c_blue}   ____) | __ | | |  __|   < ${c_reset}
    ${c_black}     `${c_green}|${c_black}____${c_green}\\${c_black}'   ${c_blue} |_____/\\____|_|  \\__/|_|\\_\\ ${c_reset}

    ${c_purple}  nf-core/sarek v${workflow.manifest.version}${c_reset}
    ${c_dim}----------------------------------------------------${c_reset}
    """.stripIndent()
}

def checkHostname(){
    def c_reset = params.monochrome_logs ? '' : "\033[0m"
    def c_white = params.monochrome_logs ? '' : "\033[0;37m"
    def c_red = params.monochrome_logs ? '' : "\033[1;91m"
    def c_yellow_bold = params.monochrome_logs ? '' : "\033[1;93m"
    if (params.hostnames){
        def hostname = "hostname".execute().text.trim()
        params.hostnames.each { prof, hnames ->
            hnames.each { hname ->
                if (hostname.contains(hname) && !workflow.profile.contains(prof)){
                    log.error "====================================================\n" +
                            "  ${c_red}WARNING!${c_reset} You are running with `-profile $workflow.profile`\n" +
                            "  but your machine hostname is ${c_white}'$hostname'${c_reset}\n" +
                            "  ${c_yellow_bold}It's highly recommended that you use `-profile $prof${c_reset}`\n" +
                            "============================================================"
                }
            }
        }
    }
}

/*
================================================================================
                                 sarek functions
================================================================================
*/

// Check if a row has the expected number of item
def checkNumberOfItem(row, number) {
    if (row.size() != number) exit 1, "Malformed row in TSV file: ${row}, see --help for more information"
    return true
}

// Check parameter existence
def checkParameterExistence(it, list) {
    if (!list.contains(it)) {
        println("Unknown parameter: ${it}")
        return false
    }
    return true
}

// Compare each parameter with a list of parameters
def checkParameterList(list, realList) {
    return list.every{ checkParameterExistence(it, realList) }
}

// Check if params.item exists and return params.genomes[params.genome].item otherwise
def checkParamReturnFile(item) {
    params."${item}" = params.genomes[params.genome]."${item}"
    return file(params."${item}")
}

// Loop through all the references files to check their existence
def checkRefExistence(referenceFile, fileToCheck) {
    if (fileToCheck instanceof List) return fileToCheck.every{ checkRefExistence(referenceFile, it) }
    def f = file(fileToCheck)
    // this is an expanded wildcard: we can assume all files exist
    if (f instanceof List && f.size() > 0) return true
    else if (!f.exists()) {
        println  "Missing references: ${referenceFile} ${fileToCheck}"
        return false
    }
    return true
}

// Loop through all the references files to check their existence
def checkReferenceMap(referenceMap) {
    referenceMap.every {
        referenceFile, fileToCheck ->
        checkRefExistence(referenceFile, fileToCheck)
    }
}

// Personnalise the MultiQC report
def createMultiQCconfig() {
  def file = workDir.resolve('multiqc_config.yaml')
  file.text  = """
  custom_logo: ${baseDir}/docs/images/Sarek_no_Border.png
  custom_logo_url: https://sarek.scilifelab.se/
  custom_logo_title: 'nf-core/sarek'
  report_header_info:

  top_modules:
  - 'fastqc'
  - 'picard'
  - 'samtools'
  - 'qualimap'
  - 'bcftools'
  - 'vcftools'
  - 'snpeff'
  """.stripIndent()

  return file
}

// Define map of reference depending of tools and step
def defineReferenceMap(step, tools) {
    def referenceMap = [
        'genomeDict'       : checkParamReturnFile("genomeDict"),
        'genomeFile'       : checkParamReturnFile("genomeFile"),
        'genomeIndex'      : checkParamReturnFile("genomeIndex"),
        'intervals'        : checkParamReturnFile("intervals")
    ]
    if ('mapping' in step) {
        referenceMap.putAll(
            'bwaIndex'         : checkParamReturnFile("bwaIndex"),
            'knownIndels'      : checkParamReturnFile("knownIndels"),
            'knownIndelsIndex' : checkParamReturnFile("knownIndelsIndex")
        )
    }
    if ('controlfreec' in tools) {
        referenceMap.putAll(
            'chrDir'           : checkParamReturnFile("chrDir"),
            'chrLength'        : checkParamReturnFile("chrLength")
        )
    }
    if ('ascat' in tools) {
        referenceMap.putAll(
            'acLoci'           : checkParamReturnFile("acLoci"),
            'acLociGC'         : checkParamReturnFile("acLociGC")
        )
    }
    if ('mapping' in step || 'haplotypecaller' in tools || 'mutect2' in tools || 'controlfreec' in tools) {
        referenceMap.putAll(
            'dbsnp'            : checkParamReturnFile("dbsnp"),
            'dbsnpIndex'       : checkParamReturnFile("dbsnpIndex")
        )
    }
    if ('annotate' in step) referenceMap = []
    return referenceMap
}

// Define list of available step
def defineStepList() {
    return [
        'annotate',
        'mapping',
        'recalibrate',
        'variantcalling'
    ]
}

// Define list of available tools
def defineToolList() {
    return [
        'ascat',
        'controlfreec',
        'freebayes',
        'haplotypecaller',
        'manta',
        'merge',
        'mpileup',
        'mutect2',
        'snpeff',
        'strelka',
        'vep'
    ]
}

// Channeling the TSV file containing BAM.
// Format is: "subject gender status sample bam bai"
def extractBams(tsvFile) {
    Channel.from(tsvFile)
        .splitCsv(sep: '\t')
        .map { row ->
            checkNumberOfItem(row, 6)
            def idPatient = row[0]
            def gender    = row[1]
            def status    = returnStatus(row[2].toInteger())
            def idSample  = row[3]
            def bamFile   = returnFile(row[4])
            def baiFile   = returnFile(row[5])

            if (!hasExtension(bamFile,"bam")) exit 1, "File: ${bamFile} has the wrong extension. See --help for more information"
            if (!hasExtension(baiFile,"bai")) exit 1, "File: ${baiFile} has the wrong extension. See --help for more information"

            return [ idPatient, gender, status, idSample, bamFile, baiFile ]
        }
}

// Create a channel of germline FASTQs from a directory pattern: "my_samples/*/"
// All FASTQ files in subdirectories are collected and emitted;
// they must have _R1_ and _R2_ in their names.
def extractFastqFromDir(pattern) {
    def fastq = Channel.create()
    // a temporary channel does all the work
    Channel
        .fromPath(pattern, type: 'dir')
        .ifEmpty { error "No directories found matching pattern '${pattern}'" }
        .subscribe onNext: { sampleDir ->
            // the last name of the sampleDir is assumed to be a unique sample id
            sampleId = sampleDir.getFileName().toString()

            for (path1 in file("${sampleDir}/**_R1_*.fastq.gz")) {
                assert path1.getName().contains('_R1_')
                path2 = file(path1.toString().replace('_R1_', '_R2_'))
                if (!path2.exists()) error "Path '${path2}' not found"
                (flowcell, lane) = flowcellLaneFromFastq(path1)
                patient = sampleId
                gender = 'ZZ'  // unused
                status = 0  // normal (not tumor)
                rgId = "${flowcell}.${sampleId}.${lane}"
                result = [patient, gender, status, sampleId, rgId, path1, path2]
                fastq.bind(result)
            }
    }, onComplete: { fastq.close() }
    fastq
}

// Extract gender from Channel as it's only used for CNVs
def extractGenders(channel) {
    def genders = [:]
    channel = channel.map{ it ->
        def idPatient = it[0]
        def gender = it[1]
        genders[idPatient] = gender
        [idPatient] + it[2..-1]
    }
    [genders, channel]
}

// Channeling the TSV file containing FASTQ or BAM
// Format is: "subject gender status sample lane fastq1 fastq2"
// or: "subject gender status sample lane bam"
def extractSample(tsvFile) {
    Channel.from(tsvFile)
        .splitCsv(sep: '\t')
        .map { row ->
            def idPatient  = row[0]
            def gender     = row[1]
            def status     = returnStatus(row[2].toInteger())
            def idSample   = row[3]
            def idRun      = row[4]
            def file1      = returnFile(row[5])
            def file2      = file("null")
            if (hasExtension(file1,"fastq.gz") || hasExtension(file1,"fq.gz")) {
                checkNumberOfItem(row, 7)
                file2 = returnFile(row[6])
            if (!hasExtension(file2,"fastq.gz") && !hasExtension(file2,"fq.gz")) exit 1, "File: ${file2} has the wrong extension. See --help for more information"
        }
        else if (hasExtension(file1,"bam")) checkNumberOfItem(row, 6)
        else "No recognisable extention for input file: ${file1}"

        [idPatient, gender, status, idSample, idRun, file1, file2]
    }
}

// Channeling the TSV file containing Recalibration Tables.
// Format is: "subject gender status sample bam bai recalTables"
def extractRecal(tsvFile) {
    Channel.from(tsvFile)
        .splitCsv(sep: '\t')
        .map { row ->
            checkNumberOfItem(row, 7)
            def idPatient  = row[0]
            def gender     = row[1]
            def status     = returnStatus(row[2].toInteger())
            def idSample   = row[3]
            def bamFile    = returnFile(row[4])
            def baiFile    = returnFile(row[5])
            def recalTable = returnFile(row[6])

            if (!hasExtension(bamFile,"bam")) exit 1, "File: ${bamFile} has the wrong extension. See --help for more information"
            if (!hasExtension(baiFile,"bai")) exit 1, "File: ${baiFile} has the wrong extension. See --help for more information"
            if (!hasExtension(recalTable,"recal.table")) exit 1, "File: ${recalTable} has the wrong extension. See --help for more information"

            [ idPatient, gender, status, idSample, bamFile, baiFile, recalTable ]
    }
}

// Parse first line of a FASTQ file, return the flowcell id and lane number.
def flowcellLaneFromFastq(path) {
    // expected format:
    // xx:yy:FLOWCELLID:LANE:... (seven fields)
    // or
    // FLOWCELLID:LANE:xx:... (five fields)
    InputStream fileStream = new FileInputStream(path.toFile())
    InputStream gzipStream = new java.util.zip.GZIPInputStream(fileStream)
    Reader decoder = new InputStreamReader(gzipStream, 'ASCII')
    BufferedReader buffered = new BufferedReader(decoder)
    def line = buffered.readLine()
    assert line.startsWith('@')
    line = line.substring(1)
    def fields = line.split(' ')[0].split(':')
    String fcid
    int lane
    if (fields.size() == 7) {
        // CASAVA 1.8+ format
        fcid = fields[2]
        lane = fields[3].toInteger()
    } else if (fields.size() == 5) {
        fcid = fields[0]
        lane = fields[1].toInteger()
    }
    [fcid, lane]
}

// Check file extension
def hasExtension(it, extension) {
    it.toString().toLowerCase().endsWith(extension.toLowerCase())
}

// Return file if it exists
def returnFile(it) {
    if (!file(it).exists()) exit 1, "Missing file in TSV file: ${it}, see --help for more information"
    return file(it)
}

// Remove .ann .gz and .vcf extension from a VCF file
def reduceVCF(file) {
    return file.fileName.toString().minus(".ann").minus(".vcf").minus(".gz")
}

// Return status [0,1]
// 0 == Normal, 1 == Tumor
def returnStatus(it) {
    if (!(it in [0, 1])) exit 1, "Status is not recognized in TSV file: ${it}, see --help for more information"
    return it
}
